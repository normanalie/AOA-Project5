{
  "report": [
    {
      "name": "lvl1_loops_coverage",
      "status": "good",
      "score": 4,
      "max_score": 4,
      "text": "Enough time of the experiment time spent in analyzed loops (100.00%)",
      "desc": "If the time spent in analyzed loops is less than 30%, standard loop optimizations will have a limited impact on application performances."
    },
    {
      "name": "lvl1_activity_ratio",
      "status": "warning",
      "score": 3,
      "max_score": 4,
      "text": "CPU activity is below 90% (88.06%)",
      "desc": "CPU cores are idle more than 10% of time. Threads supposed to run on these cores are probably IO/sync waiting. Some hints: use faster filesystems to read/write data, improve parallel load balancing and/or scheduling."
    },
    {
      "name": "lvl1_active_threads",
      "status": "warning",
      "score": 3,
      "max_score": 4,
      "text": "A significant amount of threads are idle (11.94%)",
      "desc": "On average, more than 10% of observed threads are idle. Such threads are probably IO/sync waiting. Some hints: use faster filesystems to read/write data, improve parallel load balancing and/or scheduling."
    },
    {
      "name": "lvl1_affinity_stability",
      "status": "bad",
      "score": 0,
      "max_score": 4,
      "text": "Affinity stability is lower than 90% (0.00%)",
      "desc": "Threads are often migrating to other CPU cores/threads. For OpenMP, typically set (OMP_PLACES=cores OMP_PROC_BIND=close) or (OMP_PLACES=threads OMP_PROC_BIND=spread). With OpenMPI + OpenMP, use --bind-to core --map-by node:PE=$OMP_NUM_THREADS --report-bindings. With IntelMPI + OpenMP, set I_MPI_PIN_DOMAIN=omp:compact or I_MPI_PIN_DOMAIN=omp:scatter and use -print-rank-map."
    },
    {
      "name": "lvl1_is_flat_profile",
      "status": "good",
      "score": 4,
      "max_score": 4,
      "text": "Loop profile is not flat",
      "desc": "At least one loop coverage is greater than 4% (100.00%), representing an hotspot for the application"
    },
    {
      "name": "lvl1_innermost_loops_coverage",
      "status": "good",
      "score": 4,
      "max_score": 4,
      "text": "Enough time of the experiment time spent in analyzed innermost loops (100.00%)",
      "desc": "If the time spent in analyzed innermost loops is less than 15%, standard innermost loop optimizations such as vectorisation will have a limited impact on application performances."
    },
    {
      "name": "lvl1_is_more_innermost_than_other",
      "status": "good",
      "score": 3,
      "max_score": 3,
      "text": "Cumulative Outermost/In between loops coverage (0.00%) lower than cumulative innermost loop coverage (100.00%)",
      "desc": "Having cumulative Outermost/In between loops coverage greater than cumulative innermost loop coverage will make loop optimization more complex"
    },
    {
      "name": "lvl1_blas1",
      "status": "good",
      "score": 3,
      "max_score": 3,
      "text": "Less than 10% (0.00%) is spend in BLAS1 operations",
      "desc": "It could be more efficient to inline by hand BLAS1 operations"
    },
    {
      "name": "lvl1_blas2",
      "status": "good",
      "score": 2,
      "max_score": 2,
      "text": "Less than 10% (0.00%) is spend in BLAS2 operations",
      "desc": "BLAS2 calls usually could make a poor cache usage and could benefit from inlining."
    },
    {
      "name": "lvl1_libm_usage",
      "status": "good",
      "score": 2,
      "max_score": 2,
      "text": "Less than 10% (0.00%) is spend in Libm/SVML (special functions)",
      "desc": ""
    }
  ]
}
